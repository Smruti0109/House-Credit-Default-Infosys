{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1477d374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gdown in c:\\users\\smruti deshpande\\appdata\\roaming\\python\\python310\\site-packages (5.2.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\programdata\\anaconda3\\lib\\site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from gdown) (3.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from gdown) (4.11.1)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from gdown) (4.64.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9c6a8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: dask[dataframe] in c:\\programdata\\anaconda3\\lib\\site-packages (2022.7.0)\n",
      "Requirement already satisfied: partd>=0.3.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]) (1.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]) (6.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]) (0.12.0)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]) (2.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]) (22.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]) (2022.11.0)\n",
      "Requirement already satisfied: numpy>=1.18 in c:\\users\\smruti deshpande\\appdata\\roaming\\python\\python310\\site-packages (from dask[dataframe]) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]) (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.0->dask[dataframe]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.0->dask[dataframe]) (2022.7)\n",
      "Requirement already satisfied: locket in c:\\programdata\\anaconda3\\lib\\site-packages (from partd>=0.3.10->dask[dataframe]) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.0->dask[dataframe]) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dask[dataframe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ae95fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "application_train.csv already exists.\n",
      "application_test.csv already exists.\n",
      "bureau.csv already exists.\n",
      "bureau_balance.csv already exists.\n",
      "credit_card_balance.csv already exists.\n",
      "installments_payments.csv already exists.\n",
      "previous_application.csv already exists.\n",
      "POS_CASH_balance.csv already exists.\n",
      "   SK_ID_CURR  TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR  \\\n",
      "0      100002       1         Cash loans           M            N   \n",
      "1      100003       0         Cash loans           F            N   \n",
      "2      100004       0    Revolving loans           M            Y   \n",
      "3      100006       0         Cash loans           F            N   \n",
      "4      100007       0         Cash loans           M            N   \n",
      "\n",
      "  FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
      "0               Y             0          202500.0    406597.5      24700.5   \n",
      "1               N             0          270000.0   1293502.5      35698.5   \n",
      "2               Y             0           67500.0    135000.0       6750.0   \n",
      "3               Y             0          135000.0    312682.5      29686.5   \n",
      "4               Y             0          121500.0    513000.0      21865.5   \n",
      "\n",
      "   ...  FLAG_DOCUMENT_18 FLAG_DOCUMENT_19 FLAG_DOCUMENT_20 FLAG_DOCUMENT_21  \\\n",
      "0  ...                 0                0                0                0   \n",
      "1  ...                 0                0                0                0   \n",
      "2  ...                 0                0                0                0   \n",
      "3  ...                 0                0                0                0   \n",
      "4  ...                 0                0                0                0   \n",
      "\n",
      "  AMT_REQ_CREDIT_BUREAU_HOUR AMT_REQ_CREDIT_BUREAU_DAY  \\\n",
      "0                        0.0                       0.0   \n",
      "1                        0.0                       0.0   \n",
      "2                        0.0                       0.0   \n",
      "3                        NaN                       NaN   \n",
      "4                        0.0                       0.0   \n",
      "\n",
      "   AMT_REQ_CREDIT_BUREAU_WEEK  AMT_REQ_CREDIT_BUREAU_MON  \\\n",
      "0                         0.0                        0.0   \n",
      "1                         0.0                        0.0   \n",
      "2                         0.0                        0.0   \n",
      "3                         NaN                        NaN   \n",
      "4                         0.0                        0.0   \n",
      "\n",
      "   AMT_REQ_CREDIT_BUREAU_QRT  AMT_REQ_CREDIT_BUREAU_YEAR  \n",
      "0                        0.0                         1.0  \n",
      "1                        0.0                         0.0  \n",
      "2                        0.0                         0.0  \n",
      "3                        NaN                         NaN  \n",
      "4                        0.0                         0.0  \n",
      "\n",
      "[5 rows x 122 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import gdown\n",
    "import requests\n",
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Function to construct Google Drive direct download link\n",
    "def get_google_drive_url(file_id):\n",
    "    return f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Get file IDs from the .env file\n",
    "file_ids = {\n",
    "    \"application_train\": os.getenv(\"APPLICATION_TRAIN_ID\"),\n",
    "    \"application_test\": os.getenv(\"APPLICATION_TEST_ID\"),\n",
    "    \"bureau\": os.getenv(\"BUREAU_ID\"),\n",
    "    \"bureau_balance\": os.getenv(\"BUREAU_BALANCE_ID\"),\n",
    "    \"credit_card_balance\": os.getenv(\"CREDIT_CARD_BALANCE_ID\"),\n",
    "    \"installments_payments\": os.getenv(\"INSTALLMENTS_PAYMENTS_ID\"),\n",
    "    \"previous_application\": os.getenv(\"PREVIOUS_APPLICATION_ID\"),\n",
    "    \"POS_CASH_balance\": os.getenv(\"POS_CASH_BALANCE_ID\"),\n",
    "}\n",
    "\n",
    "# Construct direct download links\n",
    "google_drive_links = {key: get_google_drive_url(value) for key, value in file_ids.items()}\n",
    "\n",
    "# Function to download a file using gdown, only if not already downloaded\n",
    "def download_csv(file_url, output_path):\n",
    "    if not os.path.exists(output_path):  # Check if file already exists\n",
    "        print(f\"Downloading {output_path}...\")\n",
    "        try:\n",
    "            gdown.download(file_url, output_path, quiet=False)\n",
    "            print(f\"Downloaded {output_path}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error downloading {file_url}: {e}\")\n",
    "    else:\n",
    "        print(f\"{output_path} already exists.\")\n",
    "\n",
    "# Define the output file paths\n",
    "output_paths = {\n",
    "    \"application_train\": \"application_train.csv\",\n",
    "    \"application_test\": \"application_test.csv\",\n",
    "    \"bureau\": \"bureau.csv\",\n",
    "    \"bureau_balance\": \"bureau_balance.csv\",\n",
    "    \"credit_card_balance\": \"credit_card_balance.csv\",\n",
    "    \"installments_payments\": \"installments_payments.csv\",\n",
    "    \"previous_application\": \"previous_application.csv\",\n",
    "    \"POS_CASH_balance\": \"POS_CASH_balance.csv\"\n",
    "}\n",
    "\n",
    "# Download the datasets\n",
    "for key, file_url in google_drive_links.items():\n",
    "    download_csv(file_url, output_paths[key])\n",
    "\n",
    "# Load datasets from local files\n",
    "try:\n",
    "    app_train = dd.read_csv(output_paths[\"application_train\"], on_bad_lines='skip')\n",
    "    app_test = dd.read_csv(output_paths[\"application_test\"], on_bad_lines='skip')\n",
    "    bureau = dd.read_csv(output_paths[\"bureau\"], on_bad_lines='skip')\n",
    "    bureau_balance = dd.read_csv(output_paths[\"bureau_balance\"], on_bad_lines='skip')\n",
    "    credit_card_balance = dd.read_csv(output_paths[\"credit_card_balance\"], on_bad_lines='skip')\n",
    "    installments_payments = dd.read_csv(output_paths[\"installments_payments\"], on_bad_lines='skip')\n",
    "    previous_application = dd.read_csv(output_paths[\"previous_application\"], on_bad_lines='skip')\n",
    "    POS_CASH_balance = dd.read_csv(output_paths[\"POS_CASH_balance\"], on_bad_lines='skip')\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV files: {e}\")\n",
    "\n",
    "# Example: Print the first few rows of the application_train dataset\n",
    "print(app_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435d78d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to reduce memory usage\n",
    "def reduce_memory_usage(df):\n",
    "    if isinstance(df, dd.DataFrame):\n",
    "        df = df.compute()  # Convert to Pandas\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:  # Exclude string columns\n",
    "            if pd.api.types.is_integer_dtype(col_type):\n",
    "                df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "            elif pd.api.types.is_float_dtype(col_type):\n",
    "                df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    return df\n",
    "\n",
    "# Load credit_card_balance using Dask\n",
    "credit_card_balance = dd.read_csv('credit_card_balance.csv', assume_missing=True)\n",
    "\n",
    "# Reduce memory usage for app_train and app_test\n",
    "app_train = reduce_memory_usage(app_train)\n",
    "app_test = reduce_memory_usage(app_test)\n",
    "\n",
    "# Select specific columns from credit_card_balance for merging\n",
    "columns_to_merge = ['SK_ID_CURR', 'AMT_BALANCE', 'SK_DPD']\n",
    "credit_card_balance_selected = credit_card_balance[columns_to_merge].compute()\n",
    "\n",
    "# Cast SK_ID_CURR to int64 to avoid type mismatches during merging\n",
    "app_train['SK_ID_CURR'] = app_train['SK_ID_CURR'].astype('int64')\n",
    "app_test['SK_ID_CURR'] = app_test['SK_ID_CURR'].astype('int64')\n",
    "credit_card_balance_selected['SK_ID_CURR'] = credit_card_balance_selected['SK_ID_CURR'].astype('int64')\n",
    "\n",
    "# Merge app_train and credit_card_balance on SK_ID_CURR\n",
    "app_train = app_train.merge(credit_card_balance_selected, on='SK_ID_CURR', how='left')\n",
    "app_test = app_test.merge(credit_card_balance_selected, on='SK_ID_CURR', how='left')\n",
    "\n",
    "# Combine datasets for processing\n",
    "merged_data = pd.concat([app_train, app_test], axis=0, ignore_index=True)\n",
    "\n",
    "# Handle missing input parameters\n",
    "input_parameters = [\n",
    "    'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_BALANCE', 'AMT_ANNUITY', 'SK_DPD', 'CNT_CHILDREN',\n",
    "    'FLAG_OWN_CAR', 'CODE_GENDER', 'DAYS_CREDIT', 'DAYS_DECISION', 'AMT_PAYMENT',\n",
    "    'AMT_INSTALMENT', 'AMT_APPLICATION', 'NAME_FAMILY_STATUS', 'NAME_INCOME_TYPE',\n",
    "    'NAME_HOUSING_TYPE', 'NAME_CONTRACT_TYPE'\n",
    "]\n",
    "\n",
    "# Add missing input parameters with NaN\n",
    "for col in input_parameters:\n",
    "    if col not in merged_data.columns:\n",
    "        merged_data[col] = np.nan\n",
    "\n",
    "# Convert FLAG_OWN_CAR from 'Y'/'N' to 1/0\n",
    "merged_data['FLAG_OWN_CAR'] = merged_data['FLAG_OWN_CAR'].map({'Y': 1, 'N': 0})\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_columns = merged_data.select_dtypes(include='number').columns.tolist()\n",
    "categorical_columns = merged_data.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# Impute missing values\n",
    "numeric_imputer = SimpleImputer(strategy='mean')\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "merged_data[numeric_columns] = numeric_imputer.fit_transform(merged_data[numeric_columns])\n",
    "\n",
    "for col in categorical_columns:\n",
    "    if col in merged_data.columns:\n",
    "        merged_data[col] = categorical_imputer.fit_transform(merged_data[[col]])\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "categorical_columns_to_encode = ['CODE_GENDER', 'NAME_CONTRACT_TYPE', 'NAME_FAMILY_STATUS', \n",
    "                                  'NAME_INCOME_TYPE', 'NAME_HOUSING_TYPE']\n",
    "merged_data = pd.get_dummies(merged_data, columns=categorical_columns_to_encode, drop_first=True)\n",
    "\n",
    "# Scale numerical features\n",
    "numerical_columns = [\n",
    "    'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_BALANCE', 'AMT_ANNUITY',\n",
    "    'SK_DPD', 'CNT_CHILDREN', 'DAYS_CREDIT', 'DAYS_DECISION',\n",
    "    'AMT_PAYMENT', 'AMT_INSTALMENT', 'AMT_APPLICATION'\n",
    "]\n",
    "\n",
    "existing_numerical_columns = [col for col in numerical_columns if col in merged_data.columns]\n",
    "scaler = MinMaxScaler()\n",
    "merged_data[existing_numerical_columns] = scaler.fit_transform(merged_data[existing_numerical_columns])\n",
    "\n",
    "# Prepare training data\n",
    "if 'TARGET' in merged_data.columns:\n",
    "    training_data = merged_data[input_parameters + ['TARGET']]\n",
    "else:\n",
    "    raise ValueError(\"TARGET column is missing from the dataset.\")\n",
    "\n",
    "X = training_data[input_parameters]\n",
    "y = training_data['TARGET']\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the first 10 rows of training data\n",
    "print(training_data.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e11cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas scikit-learn xgboost catboost lightgbm matplotlib seaborn tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544d7ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for models and preprocessing\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "# Create a function to build a DNN model for binary classification\n",
    "def create_dnn_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_dim,)))  # Define the input layer using the Input function\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize a list to hold models with preprocessing steps (imputation + scaling)\n",
    "models = {\n",
    "    'Neural Network': Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', MLPClassifier(max_iter=500))\n",
    "    ]),\n",
    "    'Random Forest': Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('classifier', RandomForestClassifier())\n",
    "    ]),\n",
    "    'Logistic Regression': Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(max_iter=500))\n",
    "    ]),\n",
    "    'XGBoost': Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n",
    "    ]),\n",
    "    'CatBoost': Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('classifier', CatBoostClassifier(silent=True))\n",
    "    ]),\n",
    "    'LightGBM': Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('classifier', LGBMClassifier())\n",
    "    ]),\n",
    "    'SVM': Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler()),  # SVM performs better with scaled data\n",
    "        ('classifier', SVC(probability=True))  # Enable probability estimates for AUC-ROC\n",
    "    ]),\n",
    "    'K-Nearest Neighbors': Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler()),  # KNN benefits from scaling\n",
    "        ('classifier', KNeighborsClassifier())\n",
    "    ]),\n",
    "    'Naive Bayes': Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('classifier', GaussianNB())  # Naive Bayes does not need scaling\n",
    "    ]),\n",
    "    'AdaBoost': Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('classifier', AdaBoostClassifier())\n",
    "    ]),\n",
    "    'Decision Tree': Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('classifier', DecisionTreeClassifier())\n",
    "    ]),\n",
    "    'DNN': create_dnn_model(input_dim=X_train.shape[1])  # Assuming your features are preprocessed\n",
    "}\n",
    "\n",
    "# Step 6: Model Building and Training\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    if model_name == 'DNN':\n",
    "        # Special handling for DNN model (sequential model instead of sklearn pipeline)\n",
    "        model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "        y_pred = (model.predict(X_test) > 0.5).astype(int)  # Convert probabilities to binary class (0 or 1)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)  # Train the model\n",
    "        y_pred = model.predict(X_test)  # Make predictions\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # AUC-ROC Curve\n",
    "    if model_name == 'DNN':  # Special handling for DNN predict_proba\n",
    "        y_pred_proba = model.predict(X_test)\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    elif hasattr(model.named_steps['classifier'], 'predict_proba'):  # Ensure model supports predict_proba\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    else:\n",
    "        roc_auc, fpr, tpr = None, None, None\n",
    "\n",
    "    # Store results\n",
    "    results[model_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'roc_auc': roc_auc,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr\n",
    "    }\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bad9d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Step 7: Plotting AUC-ROC Curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for model_name, metrics in results.items():\n",
    "    plt.plot(metrics['fpr'], metrics['tpr'], label=f'{model_name} (AUC = {metrics[\"roc_auc\"]:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014e08e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
